{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Model Evaluation on Math and Science Questions\n",
    "\n",
    "This notebook evaluates Google's Gemini model on 120 questions (60 math, 60 science) without hints.\n",
    "The model receives ONLY the questions - ground truth answers are kept completely separate until evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Environment variables loaded from .env file\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Environment variables loaded from .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini 2.0 Flash model initialized successfully using API key from .env!\n",
      "   (Using Flash model for better free tier quota limits)\n"
     ]
    }
   ],
   "source": [
    "# Configure Gemini API\n",
    "def setup_gemini():\n",
    "    \"\"\"Set up Gemini API with API key from .env file\"\"\"\n",
    "    # Get API key from environment variable\n",
    "    api_key = os.getenv('GEMINI_API_KEY')\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(\"GEMINI_API_KEY not found in .env file. Please add your API key to the .env file.\")\n",
    "    \n",
    "    genai.configure(api_key=api_key)\n",
    "    \n",
    "    # Initialize the model - using gemini-2.0-flash (fast and has better free tier limits)\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    print(\"âœ… Gemini 2.0 Flash model initialized successfully using API key from .env!\")\n",
    "    print(\"   (Using Flash model for better free tier quota limits)\")\n",
    "    return model\n",
    "\n",
    "# Initialize Gemini\n",
    "model = setup_gemini()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Questions and Separate Ground Truth\n",
    "\n",
    "**IMPORTANT**: We load the questions and immediately separate them from ground truth answers.\n",
    "The model will NEVER see the ground truth during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Loaded 60 math questions\n",
      "ðŸ”¬ Loaded 60 science questions\n",
      "\n",
      "âœ… Ground truth answers are stored separately and hidden from the model\n",
      "\n",
      "Difficulty distribution:\n",
      "  Math - Easy: 20\n",
      "  Math - Medium: 20\n",
      "  Math - Hard: 20\n",
      "  Science - Easy: 20\n",
      "  Science - Medium: 20\n",
      "  Science - Hard: 20\n"
     ]
    }
   ],
   "source": [
    "def load_questions_and_separate_ground_truth(file_path: str) -> Tuple[List[Dict], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Load questions from JSON and separate ground truth.\n",
    "    Returns:\n",
    "        - questions_only: List of questions WITHOUT ground truth\n",
    "        - ground_truth_dict: Separate dictionary of ground truth answers\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    questions_only = []\n",
    "    ground_truth_dict = {}\n",
    "    \n",
    "    for q in data['questions']:\n",
    "        # Extract ONLY the information the model should see\n",
    "        question_for_model = {\n",
    "            'id': q['id'],\n",
    "            'question': q['question'],\n",
    "            'difficulty': q['difficulty'],\n",
    "            'source': q.get('source', 'unknown')\n",
    "        }\n",
    "        \n",
    "        # Add subject for science questions\n",
    "        if 'subject' in q:\n",
    "            question_for_model['subject'] = q['subject']\n",
    "        \n",
    "        questions_only.append(question_for_model)\n",
    "        \n",
    "        # Store ground truth separately\n",
    "        ground_truth_dict[q['id']] = q['ground_truth']\n",
    "    \n",
    "    return questions_only, ground_truth_dict, data.get('metadata', {})\n",
    "\n",
    "# Load math questions\n",
    "math_questions, math_ground_truth, math_metadata = load_questions_and_separate_ground_truth(\n",
    "    '../data/no-hints/math-questions.json'\n",
    ")\n",
    "\n",
    "# Load science questions  \n",
    "science_questions, science_ground_truth, science_metadata = load_questions_and_separate_ground_truth(\n",
    "    '../data/no-hints/science-questions.json'\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Loaded {len(math_questions)} math questions\")\n",
    "print(f\"ðŸ”¬ Loaded {len(science_questions)} science questions\")\n",
    "print(f\"\\nâœ… Ground truth answers are stored separately and hidden from the model\")\n",
    "print(f\"\\nDifficulty distribution:\")\n",
    "print(f\"  Math - Easy: {sum(1 for q in math_questions if q['difficulty'] == 'easy')}\")\n",
    "print(f\"  Math - Medium: {sum(1 for q in math_questions if q['difficulty'] == 'medium')}\")\n",
    "print(f\"  Math - Hard: {sum(1 for q in math_questions if q['difficulty'] == 'hard')}\")\n",
    "print(f\"  Science - Easy: {sum(1 for q in science_questions if q['difficulty'] == 'easy')}\")\n",
    "print(f\"  Science - Medium: {sum(1 for q in science_questions if q['difficulty'] == 'medium')}\")\n",
    "print(f\"  Science - Hard: {sum(1 for q in science_questions if q['difficulty'] == 'hard')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Gemini Query Function (No Ground Truth Access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated f-string literal (detected at line 43) (1501356254.py, line 43)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated f-string literal (detected at line 43)\n"
     ]
    }
   ],
   "source": [
    "def query_gemini(question_text: str, question_type: str = 'math') -> Dict:\n",
    "    \"\"\"\n",
    "    Query Gemini with ONLY the question text.\n",
    "    The model NEVER sees the ground truth answer.\n",
    "    \"\"\"\n",
    "    if question_type == 'math':\n",
    "        prompt = f\"\"\"Solve the following math problem. Provide ONLY the final numerical answer.\n",
    "Do not include units or explanations, just the number.\n",
    "\n",
    "Problem: {question_text}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    else:  # science\n",
    "        prompt = f\"\"\"Answer the following science question. Provide ONLY the answer.\n",
    "Keep your answer concise and direct.\n",
    "\n",
    "Question: {question_text}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = model.generate_content(prompt)\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'answer': response.text.strip(),\n",
    "            'response_time': response_time,\n",
    "            'error': None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return {\n",
    "            'answer': None,\n",
    "            'response_time': None,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Test with a sample question (WITHOUT showing ground truth)\n",
    "sample_question = math_questions[0]\n",
    "print(f\"Sample Question ID {sample_question['id']}:\")\n",
    "print(f\"Question: {sample_question['question']}\")\n",
    "print(f\"\n",
    "Querying Gemini...\")\n",
    "sample_response = query_gemini(sample_question['question'], 'math')\n",
    "\n",
    "if sample_response['error']:\n",
    "    print(f\"âŒ Error: {sample_response['error']}\")\n",
    "    print(\"\n",
    "Please check your API key and internet connection.\")\n",
    "else:\n",
    "    print(f\"Gemini's Answer: {sample_response['answer']}\")\n",
    "    if sample_response['response_time']:\n",
    "        print(f\"Response Time: {sample_response['response_time']:.2f}s\")\n",
    "    print(f\"\n",
    "(Ground truth is hidden and will only be revealed during evaluation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process All Questions Through Gemini\n",
    "\n",
    "Process all 120 questions. Gemini only sees the questions, NOT the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ PROCESSING MATH QUESTIONS\n",
      "Processing 60 math questions...\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Process math questions\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ”¢ PROCESSING MATH QUESTIONS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m math_results = \u001b[43mprocess_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmath_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmath\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Process science questions\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ”¬ PROCESSING SCIENCE QUESTIONS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mprocess_questions\u001b[39m\u001b[34m(questions, question_type)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(questions, \u001b[32m1\u001b[39m):\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Query Gemini with ONLY the question text\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     response = \u001b[43mquery_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     result = {\n\u001b[32m     17\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m: q[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     18\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m: q[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m         \u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m: response[\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     23\u001b[39m     }\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msubject\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m q:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mquery_gemini\u001b[39m\u001b[34m(question_text, question_type)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     22\u001b[39m     start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     response_time = time.time() - start_time\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     27\u001b[39m         \u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m: response.text.strip(),\n\u001b[32m     28\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mresponse_time\u001b[39m\u001b[33m'\u001b[39m: response_time,\n\u001b[32m     29\u001b[39m         \u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     30\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/google/generativeai/generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    289\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    290\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    292\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    146\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/grpc/_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/grpc/_interceptor.py:329\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interceptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:79\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     64\u001b[39m     grpc_request = {\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     68\u001b[39m     }\n\u001b[32m     69\u001b[39m     _LOGGER.debug(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         },\n\u001b[32m     77\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m response = \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     81\u001b[39m     response_metadata = response.trailing_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/grpc/_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     new_method,\n\u001b[32m    308\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     new_compression,\n\u001b[32m    313\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/grpc/_channel.py:1195\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1185\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1190\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1191\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m   1192\u001b[39m     (\n\u001b[32m   1193\u001b[39m         state,\n\u001b[32m   1194\u001b[39m         call,\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/grpc/_channel.py:1162\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1145\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1146\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1147\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1161\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1163\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:62\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:58\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._interpret_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/tag.pyx.pxi:71\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._BatchOperationTag.event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/operation.pyx.pxi:138\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.ReceiveInitialMetadataOperation.un_c\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:69\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._metadata\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:70\u001b[39m, in \u001b[36mgenexpr\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:64\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._metadatum\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(_cls, key, value)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def process_questions(questions: List[Dict], question_type: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process all questions through Gemini.\n",
    "    Ground truth is NEVER passed to the model.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(questions)\n",
    "    \n",
    "    print(f\"Processing {total} {question_type} questions...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i, q in enumerate(questions, 1):\n",
    "        # Query Gemini with ONLY the question text\n",
    "        response = query_gemini(q['question'], question_type)\n",
    "        \n",
    "        result = {\n",
    "            'id': q['id'],\n",
    "            'question': q['question'],\n",
    "            'difficulty': q['difficulty'],\n",
    "            'gemini_answer': response['answer'],\n",
    "            'response_time': response['response_time'],\n",
    "            'error': response['error']\n",
    "        }\n",
    "        \n",
    "        if 'subject' in q:\n",
    "            result['subject'] = q['subject']\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Progress update\n",
    "        if i % 10 == 0 or i == total:\n",
    "            print(f\"Progress: {i}/{total} ({100*i/total:.1f}%)\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(0.5)  # Avoid API rate limits\n",
    "    \n",
    "    print(f\"âœ… Completed processing {total} {question_type} questions\\n\")\n",
    "    return results\n",
    "\n",
    "# Process math questions\n",
    "print(\"ðŸ”¢ PROCESSING MATH QUESTIONS\")\n",
    "math_results = process_questions(math_questions, 'math')\n",
    "\n",
    "# Process science questions\n",
    "print(\"ðŸ”¬ PROCESSING SCIENCE QUESTIONS\")\n",
    "science_results = process_questions(science_questions, 'science')\n",
    "\n",
    "print(\"\\nâœ… All questions processed. Gemini has provided answers without seeing ground truth.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Against Ground Truth\n",
    "\n",
    "NOW we compare Gemini's answers with the ground truth that was kept hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(answer: str, question_type: str = 'math') -> str:\n",
    "    \"\"\"\n",
    "    Normalize answers for comparison.\n",
    "    \"\"\"\n",
    "    if answer is None:\n",
    "        return \"\"\n",
    "    \n",
    "    answer = str(answer).strip().lower()\n",
    "    \n",
    "    if question_type == 'math':\n",
    "        # Extract numbers from the answer\n",
    "        numbers = re.findall(r'-?\\d+\\.?\\d*', answer)\n",
    "        if numbers:\n",
    "            # Convert to float then back to string to normalize\n",
    "            try:\n",
    "                num = float(numbers[0])\n",
    "                # If it's a whole number, return as int\n",
    "                if num.is_integer():\n",
    "                    return str(int(num))\n",
    "                return str(num)\n",
    "            except:\n",
    "                return numbers[0]\n",
    "    else:  # science\n",
    "        # Remove common words and punctuation for science answers\n",
    "        answer = re.sub(r'[^a-z0-9\\s]', '', answer)\n",
    "        # Remove articles\n",
    "        answer = re.sub(r'\\b(the|a|an)\\b', '', answer)\n",
    "        answer = ' '.join(answer.split())  # Normalize whitespace\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def evaluate_results(results: List[Dict], ground_truth: Dict[int, str], question_type: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare Gemini's answers with ground truth.\n",
    "    This is the FIRST time we look at ground truth.\n",
    "    \"\"\"\n",
    "    for result in results:\n",
    "        q_id = result['id']\n",
    "        \n",
    "        # Get ground truth (first time accessing it)\n",
    "        correct_answer = ground_truth[q_id]\n",
    "        result['ground_truth'] = correct_answer\n",
    "        \n",
    "        # Normalize both answers for comparison\n",
    "        normalized_gemini = normalize_answer(result['gemini_answer'], question_type)\n",
    "        normalized_truth = normalize_answer(correct_answer, question_type)\n",
    "        \n",
    "        # Check if correct\n",
    "        result['is_correct'] = normalized_gemini == normalized_truth\n",
    "        result['normalized_gemini'] = normalized_gemini\n",
    "        result['normalized_truth'] = normalized_truth\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate math results\n",
    "math_df = evaluate_results(math_results, math_ground_truth, 'math')\n",
    "print(\"ðŸ“Š Math Results Evaluated\")\n",
    "print(f\"Correct: {math_df['is_correct'].sum()}/{len(math_df)}\")\n",
    "print(f\"Accuracy: {100 * math_df['is_correct'].mean():.2f}%\\n\")\n",
    "\n",
    "# Evaluate science results\n",
    "science_df = evaluate_results(science_results, science_ground_truth, 'science')\n",
    "print(\"ðŸ”¬ Science Results Evaluated\")\n",
    "print(f\"Correct: {science_df['is_correct'].sum()}/{len(science_df)}\")\n",
    "print(f\"Accuracy: {100 * science_df['is_correct'].mean():.2f}%\\n\")\n",
    "\n",
    "# Combine all results\n",
    "math_df['domain'] = 'math'\n",
    "science_df['domain'] = 'science'\n",
    "all_results_df = pd.concat([math_df, science_df], ignore_index=True)\n",
    "\n",
    "print(\"ðŸ“ˆ Overall Results\")\n",
    "print(f\"Total Correct: {all_results_df['is_correct'].sum()}/{len(all_results_df)}\")\n",
    "print(f\"Overall Accuracy: {100 * all_results_df['is_correct'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy metrics\n",
    "def calculate_metrics(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Calculate various accuracy metrics\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Overall metrics\n",
    "    metrics['overall_accuracy'] = df['is_correct'].mean() * 100\n",
    "    metrics['total_correct'] = df['is_correct'].sum()\n",
    "    metrics['total_questions'] = len(df)\n",
    "    \n",
    "    # By domain\n",
    "    metrics['by_domain'] = df.groupby('domain')['is_correct'].agg(['mean', 'sum', 'count'])\n",
    "    metrics['by_domain']['mean'] *= 100\n",
    "    \n",
    "    # By difficulty\n",
    "    metrics['by_difficulty'] = df.groupby('difficulty')['is_correct'].agg(['mean', 'sum', 'count'])\n",
    "    metrics['by_difficulty']['mean'] *= 100\n",
    "    \n",
    "    # By difficulty and domain\n",
    "    metrics['by_difficulty_domain'] = df.groupby(['difficulty', 'domain'])['is_correct'].mean() * 100\n",
    "    \n",
    "    # For science, by subject\n",
    "    science_df = df[df['domain'] == 'science']\n",
    "    if 'subject' in science_df.columns:\n",
    "        metrics['by_subject'] = science_df.groupby('subject')['is_correct'].agg(['mean', 'sum', 'count'])\n",
    "        metrics['by_subject']['mean'] *= 100\n",
    "    \n",
    "    # Response time analysis\n",
    "    metrics['avg_response_time'] = df['response_time'].mean()\n",
    "    metrics['total_time'] = df['response_time'].sum()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "metrics = calculate_metrics(all_results_df)\n",
    "\n",
    "# Display comprehensive metrics\n",
    "print(\"=\"*60)\n",
    "print(\"COMPREHENSIVE PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸ“Š OVERALL PERFORMANCE\")\n",
    "print(f\"  Total Questions: {metrics['total_questions']}\")\n",
    "print(f\"  Correct Answers: {metrics['total_correct']}\")\n",
    "print(f\"  Accuracy: {metrics['overall_accuracy']:.2f}%\")\n",
    "print(f\"  Average Response Time: {metrics['avg_response_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ ACCURACY BY DOMAIN\")\n",
    "display(metrics['by_domain'])\n",
    "\n",
    "print(f\"\\nðŸ“Š ACCURACY BY DIFFICULTY\")\n",
    "display(metrics['by_difficulty'])\n",
    "\n",
    "print(f\"\\nðŸ”¬ SCIENCE ACCURACY BY SUBJECT\")\n",
    "if 'by_subject' in metrics:\n",
    "    display(metrics['by_subject'])\n",
    "\n",
    "print(f\"\\nðŸ“‹ ACCURACY BY DIFFICULTY AND DOMAIN\")\n",
    "display(metrics['by_difficulty_domain'].unstack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Overall Accuracy Comparison\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "domains = ['Overall', 'Math', 'Science']\n",
    "accuracies = [\n",
    "    metrics['overall_accuracy'],\n",
    "    metrics['by_domain'].loc['math', 'mean'],\n",
    "    metrics['by_domain'].loc['science', 'mean']\n",
    "]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = ax1.bar(domains, accuracies, color=colors)\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('Overall Accuracy Comparison')\n",
    "ax1.set_ylim(0, 100)\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{acc:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# 2. Accuracy by Difficulty\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "difficulties = ['easy', 'medium', 'hard']\n",
    "diff_acc = [metrics['by_difficulty'].loc[d, 'mean'] for d in difficulties]\n",
    "bars = ax2.bar(difficulties, diff_acc, color=['#2ecc71', '#f39c12', '#e74c3c'])\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Accuracy by Difficulty Level')\n",
    "ax2.set_ylim(0, 100)\n",
    "for bar, acc in zip(bars, diff_acc):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{acc:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# 3. Grouped Bar Chart - Difficulty by Domain\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "x = np.arange(len(difficulties))\n",
    "width = 0.35\n",
    "math_accs = [metrics['by_difficulty_domain'].loc[(d, 'math')] for d in difficulties]\n",
    "science_accs = [metrics['by_difficulty_domain'].loc[(d, 'science')] for d in difficulties]\n",
    "\n",
    "ax3.bar(x - width/2, math_accs, width, label='Math', color='#e74c3c')\n",
    "ax3.bar(x + width/2, science_accs, width, label='Science', color='#2ecc71')\n",
    "ax3.set_xlabel('Difficulty')\n",
    "ax3.set_ylabel('Accuracy (%)')\n",
    "ax3.set_title('Accuracy by Difficulty and Domain')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(difficulties)\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 100)\n",
    "\n",
    "# 4. Pie Chart - Correct vs Incorrect\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "sizes = [metrics['total_correct'], metrics['total_questions'] - metrics['total_correct']]\n",
    "labels = [f'Correct\\n({sizes[0]})', f'Incorrect\\n({sizes[1]})']\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax4.set_title('Overall Correct vs Incorrect')\n",
    "\n",
    "# 5. Science Subject Performance\n",
    "if 'by_subject' in metrics:\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    subjects = metrics['by_subject'].index.tolist()\n",
    "    subject_accs = metrics['by_subject']['mean'].tolist()\n",
    "    bars = ax5.bar(subjects, subject_accs, color=['#9b59b6', '#3498db', '#e67e22'])\n",
    "    ax5.set_ylabel('Accuracy (%)')\n",
    "    ax5.set_title('Science Accuracy by Subject')\n",
    "    ax5.set_ylim(0, 100)\n",
    "    for bar, acc in zip(bars, subject_accs):\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                 f'{acc:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# 6. Response Time Distribution\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "ax6.hist(all_results_df['response_time'].dropna(), bins=30, color='#3498db', alpha=0.7)\n",
    "ax6.axvline(metrics['avg_response_time'], color='red', linestyle='--', \n",
    "            label=f'Mean: {metrics[\"avg_response_time\"]:.2f}s')\n",
    "ax6.set_xlabel('Response Time (seconds)')\n",
    "ax6.set_ylabel('Frequency')\n",
    "ax6.set_title('Response Time Distribution')\n",
    "ax6.legend()\n",
    "\n",
    "# 7. Heatmap - Performance Matrix\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "heatmap_data = metrics['by_difficulty_domain'].unstack()\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "            vmin=0, vmax=100, ax=ax7, cbar_kws={'label': 'Accuracy (%)'})\n",
    "ax7.set_title('Performance Heatmap: Difficulty Ã— Domain')\n",
    "ax7.set_xlabel('Domain')\n",
    "ax7.set_ylabel('Difficulty')\n",
    "\n",
    "# 8. Question-by-Question Performance (first 30 questions)\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "sample_size = 30\n",
    "sample_df = all_results_df.head(sample_size)\n",
    "x_pos = np.arange(sample_size)\n",
    "colors_map = {True: '#2ecc71', False: '#e74c3c'}\n",
    "bar_colors = [colors_map[val] for val in sample_df['is_correct']]\n",
    "ax8.bar(x_pos, [1]*sample_size, color=bar_colors, alpha=0.7)\n",
    "ax8.set_xlabel('Question Number')\n",
    "ax8.set_ylabel('Correct (Green) / Incorrect (Red)')\n",
    "ax8.set_title(f'Performance on First {sample_size} Questions')\n",
    "ax8.set_ylim(0, 1.2)\n",
    "ax8.set_yticks([])\n",
    "\n",
    "# 9. Cumulative Accuracy Over Time\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "cumulative_correct = all_results_df['is_correct'].cumsum()\n",
    "cumulative_total = np.arange(1, len(all_results_df) + 1)\n",
    "cumulative_accuracy = (cumulative_correct / cumulative_total) * 100\n",
    "ax9.plot(cumulative_accuracy, color='#3498db', linewidth=2)\n",
    "ax9.axhline(y=metrics['overall_accuracy'], color='red', linestyle='--', \n",
    "            alpha=0.5, label=f'Final: {metrics[\"overall_accuracy\"]:.1f}%')\n",
    "ax9.set_xlabel('Question Number')\n",
    "ax9.set_ylabel('Cumulative Accuracy (%)')\n",
    "ax9.set_title('Cumulative Accuracy Over All Questions')\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Gemini Model Performance Analysis', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze incorrect answers\n",
    "incorrect_df = all_results_df[~all_results_df['is_correct']]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\n",
    "ðŸ“Š Error Statistics:\")\n",
    "print(f\"Total Errors: {len(incorrect_df)}\")\n",
    "print(f\"Error Rate: {100 * len(incorrect_df) / len(all_results_df):.2f}%\")\n",
    "\n",
    "print(f\"\n",
    "ðŸ“ˆ Errors by Domain:\")\n",
    "error_by_domain = incorrect_df['domain'].value_counts()\n",
    "for domain, count in error_by_domain.items():\n",
    "    total = len(all_results_df[all_results_df['domain'] == domain])\n",
    "    print(f\"  {domain.capitalize()}: {count}/{total} ({100*count/total:.1f}% error rate)\")\n",
    "\n",
    "print(f\"\n",
    "ðŸ“Š Errors by Difficulty:\")\n",
    "error_by_difficulty = incorrect_df['difficulty'].value_counts()\n",
    "for diff, count in error_by_difficulty.items():\n",
    "    total = len(all_results_df[all_results_df['difficulty'] == diff])\n",
    "    print(f\"  {diff.capitalize()}: {count}/{total} ({100*count/total:.1f}% error rate)\")\n",
    "\n",
    "print(f\"\n",
    "ðŸ”¬ Science Errors by Subject:\")\n",
    "science_errors = incorrect_df[incorrect_df['domain'] == 'science']\n",
    "if 'subject' in science_errors.columns:\n",
    "    error_by_subject = science_errors['subject'].value_counts()\n",
    "    for subject, count in error_by_subject.items():\n",
    "        total = len(science_df[science_df['subject'] == subject])\n",
    "        print(f\"  {subject.capitalize()}: {count}/{total} ({100*count/total:.1f}% error rate)\")\n",
    "\n",
    "# Show sample of errors\n",
    "print(\"\n",
    "\" + \"=\"*60)\n",
    "print(\"SAMPLE OF INCORRECT ANSWERS (First 5)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx, row in incorrect_df.head(5).iterrows():\n",
    "    print(f\"\n",
    "âŒ Question {row['id']} ({row['domain']}, {row['difficulty']}):\")\n",
    "    question_text = row['question'][:100] + \"...\" if len(row['question']) > 100 else row['question']\n",
    "    print(f\"   Question: {question_text}\")\n",
    "    print(f\"   Gemini Answer: {row['gemini_answer']}\")\n",
    "    print(f\"   Correct Answer: {row['ground_truth']}\")\n",
    "    print(f\"   Normalized Comparison: '{row['normalized_gemini']}' vs '{row['normalized_truth']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to CSV\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"gemini_evaluation_results_{timestamp}.csv\"\n",
    "\n",
    "# Prepare export dataframe\n",
    "export_df = all_results_df[[\n",
    "    'id', 'domain', 'difficulty', 'question', \n",
    "    'gemini_answer', 'ground_truth', 'is_correct',\n",
    "    'response_time'\n",
    "]].copy()\n",
    "\n",
    "# Add subject for science questions\n",
    "if 'subject' in all_results_df.columns:\n",
    "    export_df['subject'] = all_results_df['subject']\n",
    "\n",
    "# Save to CSV\n",
    "export_df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Results saved to: {output_file}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = f\"\"\"\n",
    "GEMINI MODEL EVALUATION SUMMARY\n",
    "================================\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Model: Gemini 1.5 Pro\n",
    "\n",
    "OVERALL RESULTS\n",
    "---------------\n",
    "Total Questions: {metrics['total_questions']}\n",
    "Correct Answers: {metrics['total_correct']}\n",
    "Overall Accuracy: {metrics['overall_accuracy']:.2f}%\n",
    "Average Response Time: {metrics['avg_response_time']:.2f} seconds\n",
    "Total Processing Time: {metrics['total_time']/60:.2f} minutes\n",
    "\n",
    "PERFORMANCE BY DOMAIN\n",
    "--------------------\n",
    "Math Accuracy: {metrics['by_domain'].loc['math', 'mean']:.2f}% ({int(metrics['by_domain'].loc['math', 'sum'])}/{int(metrics['by_domain'].loc['math', 'count'])})\n",
    "Science Accuracy: {metrics['by_domain'].loc['science', 'mean']:.2f}% ({int(metrics['by_domain'].loc['science', 'sum'])}/{int(metrics['by_domain'].loc['science', 'count'])})\n",
    "\n",
    "PERFORMANCE BY DIFFICULTY\n",
    "------------------------\n",
    "Easy: {metrics['by_difficulty'].loc['easy', 'mean']:.2f}% ({int(metrics['by_difficulty'].loc['easy', 'sum'])}/{int(metrics['by_difficulty'].loc['easy', 'count'])})\n",
    "Medium: {metrics['by_difficulty'].loc['medium', 'mean']:.2f}% ({int(metrics['by_difficulty'].loc['medium', 'sum'])}/{int(metrics['by_difficulty'].loc['medium', 'count'])})\n",
    "Hard: {metrics['by_difficulty'].loc['hard', 'mean']:.2f}% ({int(metrics['by_difficulty'].loc['hard', 'sum'])}/{int(metrics['by_difficulty'].loc['hard', 'count'])})\n",
    "\n",
    "KEY FINDINGS\n",
    "-----------\n",
    "â€¢ Best Performance: {('Math' if metrics['by_domain'].loc['math', 'mean'] > metrics['by_domain'].loc['science', 'mean'] else 'Science')} domain\n",
    "â€¢ Most Challenging: {'Hard' if metrics['by_difficulty'].loc['hard', 'mean'] < metrics['by_difficulty'].loc['medium', 'mean'] and metrics['by_difficulty'].loc['hard', 'mean'] < metrics['by_difficulty'].loc['easy', 'mean'] else ('Medium' if metrics['by_difficulty'].loc['medium', 'mean'] < metrics['by_difficulty'].loc['easy', 'mean'] else 'Easy')} difficulty level\n",
    "â€¢ Error Rate: {100 - metrics['overall_accuracy']:.2f}%\n",
    "\n",
    "Note: Ground truth was completely isolated from the model during inference.\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "summary_file = f\"gemini_evaluation_summary_{timestamp}.txt\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\nðŸ“„ Summary report saved to: {summary_file}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(summary_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final insights\n",
    "display(Markdown(\"# ðŸŽ¯ Final Evaluation Summary\"))\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "## Key Performance Indicators\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Overall Accuracy** | {metrics['overall_accuracy']:.2f}% |\n",
    "| **Math Accuracy** | {metrics['by_domain'].loc['math', 'mean']:.2f}% |\n",
    "| **Science Accuracy** | {metrics['by_domain'].loc['science', 'mean']:.2f}% |\n",
    "| **Easy Questions** | {metrics['by_difficulty'].loc['easy', 'mean']:.2f}% |\n",
    "| **Medium Questions** | {metrics['by_difficulty'].loc['medium', 'mean']:.2f}% |\n",
    "| **Hard Questions** | {metrics['by_difficulty'].loc['hard', 'mean']:.2f}% |\n",
    "| **Avg Response Time** | {metrics['avg_response_time']:.2f}s |\n",
    "\n",
    "## ðŸ“Š Performance Analysis\n",
    "\n",
    "### Strengths:\n",
    "- Best performance on **{('Math' if metrics['by_domain'].loc['math', 'mean'] > metrics['by_domain'].loc['science', 'mean'] else 'Science')}** questions\n",
    "- Highest accuracy on **{('Easy' if metrics['by_difficulty'].loc['easy', 'mean'] > metrics['by_difficulty'].loc['medium', 'mean'] and metrics['by_difficulty'].loc['easy', 'mean'] > metrics['by_difficulty'].loc['hard', 'mean'] else ('Medium' if metrics['by_difficulty'].loc['medium', 'mean'] > metrics['by_difficulty'].loc['hard', 'mean'] else 'Hard'))}** difficulty level\n",
    "- Consistent response time averaging {metrics['avg_response_time']:.2f} seconds\n",
    "\n",
    "### Areas for Improvement:\n",
    "- Lower performance on **{('Math' if metrics['by_domain'].loc['math', 'mean'] < metrics['by_domain'].loc['science', 'mean'] else 'Science')}** questions\n",
    "- Struggles with **{('Hard' if metrics['by_difficulty'].loc['hard', 'mean'] < metrics['by_difficulty'].loc['medium', 'mean'] and metrics['by_difficulty'].loc['hard', 'mean'] < metrics['by_difficulty'].loc['easy', 'mean'] else ('Medium' if metrics['by_difficulty'].loc['medium', 'mean'] < metrics['by_difficulty'].loc['easy', 'mean'] else 'Easy'))}** difficulty questions\n",
    "- Overall error rate of {100 - metrics['overall_accuracy']:.2f}%\n",
    "\n",
    "## ðŸ” Evaluation Integrity\n",
    "\n",
    "âœ… **Ground Truth Isolation Verified**: \n",
    "- Questions were separated from answers before processing\n",
    "- Gemini only received question text, never the correct answers\n",
    "- Ground truth was only accessed during the evaluation phase\n",
    "- This ensures unbiased model performance measurement\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete! All results have been saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
